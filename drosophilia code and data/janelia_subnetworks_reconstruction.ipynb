{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d3c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install navis for importing navis library to download neuron .swc files of drosophilia\n",
    "import navis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f801f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neuprint wrapper by navis\n",
    "import navis.interfaces.neuprint as neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d201622",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6ImthcmFua2htOUBnbWFpbC5jb20iLCJsZXZlbCI6Im5vYXV0aCIsImltYWdlLXVybCI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hL0FBY0hUdGRrUEVpZTNEWlhrdHFIVkZsRDB1cU91VE1MUWhNMndpZ0VKcFMtZmJQTFJndz1zOTYtYz9zej01MD9zej01MCIsImV4cCI6MTg2ODIxNjU1M30.SdLfF_hDB4BGQqCsb0IDu_umQ11kS9q5qFbxL_I0Q7I\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = neu.Client('https://neuprint.janelia.org/', dataset='hemibrain:v1.2.1',token=token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d74988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect SWC neuron attributes data for each neuron. These neurons are grouped according to each of the 8 modules \n",
    "# of EM drosophilia network given by averageSubComm(num)_drosophilia.csv where (num) takes value 0 to 7. \n",
    "\n",
    "for j in range(8):\n",
    "    print(\"Comm %d\"%j)\n",
    "    nodes = pd.read_csv(\"./EM_communities/averageSubComm%d_drosophilia.csv\"%j).Node.to_list()\n",
    "\n",
    "    neuron_attrDict = {\"type\":[], \"name\":[], \"id\":[], \n",
    "                   \"n_connectors\": [], \"n_branches\":[], \n",
    "                   \"n_leafs\":[], \"cable_length\":[], \n",
    "                    \"soma\":[], \"units\":[]}\n",
    "\n",
    "    directory = './janelia_Comm%d'%j+'/swc'\n",
    "\n",
    "    # Check if the directory already exists\n",
    "    if not os.path.exists(directory):\n",
    "        # Create the directory\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    \n",
    "    for i in range(len(nodes)):\n",
    "        # use heal=True to get connected tree \n",
    "        n = neu.fetch_skeletons(nodes[i],heal=True, client=client)\n",
    "\n",
    "        neuron_attrDict[\"type\"].append(n[0].type)\n",
    "        neuron_attrDict[\"name\"].append(n[0].name)\n",
    "        neuron_attrDict[\"id\"].append(n[0].id)\n",
    "\n",
    "        neuron_attrDict[\"n_connectors\"].append(n[0].n_connectors)\n",
    "        neuron_attrDict[\"n_branches\"].append(n[0].n_branches)\n",
    "        neuron_attrDict[\"n_leafs\"].append(n[0].n_leafs)\n",
    "        neuron_attrDict[\"cable_length\"].append(n[0].cable_length)\n",
    "        neuron_attrDict[\"soma\"].append(n[0].soma)\n",
    "        neuron_attrDict[\"units\"].append(str(n[0].units))\n",
    "\n",
    "        n.nodes.to_csv(directory+\"/%d.swc\"%nodes[i], index=False)\n",
    "\n",
    "        if (i+1)%500==0:\n",
    "            print(i+1)\n",
    "\n",
    "\n",
    "    df_neuron_attr = pd.DataFrame(neuron_attrDict)\n",
    "\n",
    "    # store the attributes of each neuron corresponding to a given module as neu_attrJanelia.csv file\n",
    "    # in the folder janelia_Comm(num). where (num) takes value 0 to 7\n",
    "    df_neuron_attr.to_csv(\"./janelia_Comm%d/neu_attrJanelia.csv\"%j,index=False)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e554fbbf-2bd3-4dcf-bce6-462b18039f39",
   "metadata": {},
   "source": [
    "#### We make an educated assumption here. There can exist neuron without soma information. In that case........\n",
    "#### Assumption: if there exists no soma, then there are no dendritic endpoints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a42ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given neuron swc file neuromorphological data as input, construct networkx graph object\n",
    "def get_neuron_graph(neuron):\n",
    "    #neuron is a df\n",
    "    \n",
    "    # source nodes\n",
    "    source = neuron.parent_id.tolist()[1:]\n",
    "    \n",
    "    #print(len(source))\n",
    "\n",
    "    # target nodes(actually neurites)\n",
    "    target = neuron.node_id.tolist()[1:]\n",
    "    \n",
    "    initial = min(np.concatenate((source,target)))\n",
    "    final = max(np.concatenate((source,target)))\n",
    "    nodes = neuron.node_id.tolist()\n",
    "    \n",
    "    \n",
    "    nodes_list = []\n",
    "    for i in nodes:\n",
    "        x = neuron[neuron.node_id==i].x\n",
    "        y = neuron[neuron.node_id==i].y\n",
    "        z = neuron[neuron.node_id==i].z\n",
    "        r = neuron[neuron.node_id==i].radius\n",
    "       \n",
    "        nodes_list.append((i,{\"pos\":(x,y,z),\"radius\":r  }))\n",
    "        \n",
    "    edge_list = []\n",
    "    for i,j in zip(source, target):\n",
    "        edge_list.append( (i,j) )\n",
    "    \n",
    "    global G\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add all the nodes to the graph\n",
    "    G.add_nodes_from(nodes_list)\n",
    "    # Add all the edges to the graph\n",
    "    G.add_edges_from(edge_list)\n",
    "    \n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc3157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf5b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the string in alphanumeric order\n",
    "import re\n",
    "\n",
    "def sorted_alphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
    "    return sorted(data, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downstream refers to directed arrow links in a tree/river going from the single main source to its tributaries\n",
    "\n",
    "# for neurons of zebrafish, since the directed arrow goes from soma source/parent to child we take downstream defn.\n",
    "def strahler_number_downstream(node):\n",
    "    # -- If the node is a leaf (has no children), its Strahler number is one.\n",
    "    # https://stackoverflow.com/questions/29202822/how-to-determine-strahler-number-on-a-directed-graph-for-a-stream-network\n",
    "    if G.out_degree(node)==0:\n",
    "        return 1\n",
    "    \n",
    "    # If a node has only one child, it is assigned the strahler number of its child.\n",
    "    # see http://selezovikj.blogspot.com/2011/11/strahler-number-of-binary-tree.html\n",
    "    elif G.out_degree(node)==1:\n",
    "        \n",
    "        child = list((G.out_edges(node)))[0][1]\n",
    "        \n",
    "        return strahler_number_downstream(child)\n",
    "    \n",
    "    elif G.out_degree(node)>=2:\n",
    "        children = [edge[1] for edge in list(G.out_edges(node))] # small difference here wrt \"upstream\" definition\n",
    "        strahler_order = list(map(strahler_number_downstream,children))\n",
    "        #If the node has one child with Strahler number i, \n",
    "        #            -- and all other children have Strahler numbers less than i, \n",
    "        #            -- then the Strahler number of the node is i again.\n",
    "        if min(strahler_order) < max(strahler_order):\n",
    "            return max(strahler_order)\n",
    "\n",
    "        #-- If the node has two or more children with Strahler number i, \n",
    "        #-- and no children with greater number, \n",
    "        #-- then the Strahler number of the node is i + 1.\n",
    "        if min(strahler_order) == max(strahler_order):\n",
    "            return max(strahler_order) + 1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f96e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes the dataframe consisting of neuromorphological information, position coordinates of branch points \n",
    "# of a neuron. computes the strahler number for each branch point of neuron tree and saves the strahler number\n",
    "# as a column corresponding to each branch point.\n",
    "\n",
    "def add_strahler_to_swc(path, swc_dir, new_swc_dir, swc_file):\n",
    "\n",
    "    neuron_name = swc_file[:-4]\n",
    "    \n",
    "    \n",
    "    #print(neuron_name)\n",
    "    neuron_df = pd.read_csv(path+swc_dir+\"%s\"%swc_file)\n",
    "    \n",
    "    \n",
    "    G = get_neuron_graph(neuron_df)\n",
    "    # G is global variable here\n",
    "    \n",
    "    strahler_dict = {}        \n",
    "    for node in list(G.nodes()):\n",
    "        strahler_dict[node] = strahler_number_downstream(node)\n",
    "\n",
    "        \n",
    "    #strahler_dict        \n",
    "        \n",
    "    correct_strahler_order = np.array([ strahler_dict[ i ] for i in neuron_df.node_id.tolist() ])\n",
    "    \n",
    "    neuron_df[\"strah_num\"] = correct_strahler_order\n",
    "    \n",
    "    \n",
    "    neuron_df.to_csv(path+new_swc_dir+swc_file,index=False)\n",
    "    G.clear() # remove all nodes and edges\n",
    "    #return neuron_df    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971801c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b782bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of the 8 communities that groups the neurons, compute strahler number for each branch point of a neuron\n",
    "# and finally store it in the format of swc file, with last column being strahler order.\n",
    "\n",
    "for j in range(8):\n",
    "    path = \"./janelia_Comm%d/\"%j\n",
    "    swc_dir = \"swc/\"\n",
    "    new_swc_dir = \"janelia dataset with strahler lastcolumn/\"\n",
    "    \n",
    "    if not os.path.exists(path+new_swc_dir):\n",
    "      \n",
    "        # if the demo_folder directory is not present \n",
    "        # then create it.\n",
    "        os.makedirs(path+new_swc_dir)\n",
    "\n",
    "    file_path = path+swc_dir\n",
    "    file_list=[f for f in sorted_alphanumeric(os.listdir(file_path)) if f.endswith('.swc')\n",
    "                    and os.path.isfile(os.path.join(file_path, f))]\n",
    "    \n",
    "    print(\"Comm %d\"%j)\n",
    "    start = time.time()\n",
    "    i=0    \n",
    "    for swc_file in file_list:\n",
    "        #print(swc_file)\n",
    "        add_strahler_to_swc(path, swc_dir, new_swc_dir, swc_file)\n",
    "        i+=1\n",
    "        if (i%500==0):\n",
    "            print(i)\n",
    "    end = time.time()\n",
    "    print(\"Total time: %.2f min\\n\"%((end-start)/60) )\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62cc915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa912ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalling assumption: if a node doesn't have soma, then it doesn't have dendritic terminals\n",
    "# so there won't be any receivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16954b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold value is real number which separates the axoonal terminals from dendritic endpoints\n",
    "# assuming that the soma to neurites(terminals endpoints/branch points whose strahler number = 1) \n",
    "# distribution is mostly bimodal.\n",
    "def compute_threshold(y):\n",
    "    bins = np.histogram_bin_edges(y,bins=\"sturges\")\n",
    "    freq = (plt.hist(y,bins=bins))[0]\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    # compute threshold value\n",
    "    new_freq = deepcopy(freq[1:len(freq)-1])\n",
    "    index = np.argmin(new_freq)+1\n",
    "    \n",
    "    thresh_value = (bins[index]+bins[index+1])/2\n",
    "\n",
    "    return thresh_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7100c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# compute threshold value for each neuron grouped by their communities\n",
    "# finally save it as attribute in the attribute file for each community.\n",
    "for j in range(8):\n",
    "    print(\"Comm %d\"%j)\n",
    "    path = \"./janelia_Comm%d/\"%j\n",
    "    new_swc_dir = \"janelia dataset with strahler lastcolumn/\"\n",
    "    \n",
    "    df_neuron_attr = pd.read_csv(path+\"neu_attrJanelia.csv\")\n",
    "    node_wSoma = df_neuron_attr[~np.isnan( df_neuron_attr.soma )].id.tolist()\n",
    "\n",
    "    thresh_val = []\n",
    "    for node in df_neuron_attr.id.tolist():\n",
    "        if node in node_wSoma:\n",
    "\n",
    "            df = pd.read_csv(path+new_swc_dir+\"%s.swc\"%str(node))\n",
    "            soma_id = int(df_neuron_attr.loc[df_neuron_attr.id==node, \"soma\"].tolist()[0])\n",
    "\n",
    "            soma_index = df[df.node_id==soma_id].index[0]\n",
    "\n",
    "            soma_xyz = np.array( df.loc[soma_index, [\"x\",\"y\",\"z\"]].tolist() )\n",
    "            soma_xyz = np.array( df.loc[soma_index, [\"x\",'y',\"z\"]].tolist() )\n",
    "            soma_xyz = np.array([ [i for i in soma_xyz] ])\n",
    "\n",
    "            # remove soma from df\n",
    "            df = df.drop(soma_index)\n",
    "            # remove all SN>1 from df\n",
    "            df = df[df.strah_num==1]\n",
    "\n",
    "            neurites_xyz = np.array( df.loc[:, [\"x\",\"y\",\"z\"]] )\n",
    "\n",
    "            dist = distance.cdist(soma_xyz, neurites_xyz, 'euclidean')[0]\n",
    "\n",
    "            thresh = compute_threshold(dist)\n",
    "\n",
    "            thresh_val.append(thresh)\n",
    "\n",
    "        else:\n",
    "            thresh_val.append(float(\"nan\"))\n",
    "    \n",
    "    df_neuron_attr[\"thresh_val\"]=thresh_val\n",
    "    df_neuron_attr.to_csv(path+\"neu_attrJanelia_updated.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d79f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for each community of neurons considered, the percentage of neurons without soma.\n",
    "\n",
    "print(\"Module \\t   No. of neurons   Percentage of neurons without soma:\\n\")\n",
    "for j in range(8):\n",
    "\n",
    "    df = pd.read_csv(\"./janelia_Comm%d/neu_attrJanelia_updated.csv\"%j)\n",
    "    lis = df.soma.isna().tolist()\n",
    "\n",
    "    percent = np.sum(lis)*100/len(df)\n",
    "    \n",
    "    print(\"Module %d \\t %d \\t %.2f %%\" %(j, len(df) ,percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given neuron node and its position coordinates of the terminal endpoints(branch points with strahler numebr 1)\n",
    "# using threshold distance stored in the neuron attributes csv file, it separates the terminal endpoints\n",
    "# into axon terminals and dendritic endpoints.\n",
    "\n",
    "def get_endpoints(node, df_neuron_attr, fpath):\n",
    "    df = pd.read_csv(fpath+\"%s.swc\"%str(node))\n",
    "    \n",
    "    soma_id = df_neuron_attr.loc[df_neuron_attr.id==node, \"soma\"].tolist()[0]\n",
    "    \n",
    "    if np.isnan( soma_id ):\n",
    "        \n",
    "        dendrite_xyz = np.zeros((1,3))#float(\"nan\")\n",
    "        axon_xyz = np.array( df.loc[:, [\"x\",\"y\",\"z\"]] )\n",
    "        if len(axon_xyz)==0:\n",
    "            axon_xyz=np.zeros((1,3))#float(\"nan\")\n",
    "        \n",
    "        return(dendrite_xyz, axon_xyz)\n",
    "\n",
    "    else:\n",
    "        soma_id = int(soma_id)\n",
    "        soma_index = df[df.node_id==soma_id].index[0]\n",
    "        soma_xyz = np.array( df.loc[soma_index, [\"x\",\"y\",\"z\"]].tolist() )\n",
    "        soma_xyz = np.array( df.loc[soma_index, [\"x\",'y',\"z\"]].tolist() )\n",
    "        soma_xyz = np.array([ [i for i in soma_xyz] ])\n",
    "\n",
    "        # remove soma from df\n",
    "        df = df.drop(soma_index)\n",
    "        # remove all SN>1 from df\n",
    "        df = df[df.strah_num==1]\n",
    "\n",
    "        neurites_xyz = np.array( df.loc[:, [\"x\",\"y\",\"z\"]] )\n",
    "        thresh_val = df_neuron_attr[df_neuron_attr.id==node]['thresh_val'].tolist()[0]\n",
    "        \n",
    "\n",
    "        neurites_xyz = np.array( df.loc[:, [\"x\",\"y\",\"z\"]] )\n",
    "\n",
    "        dist = distance.cdist(soma_xyz, neurites_xyz, 'euclidean')[0]\n",
    "        #print(dist.shape, thresh_val)\n",
    "        dendrite_xyz = neurites_xyz[dist<=thresh_val]\n",
    "        axon_xyz = neurites_xyz[dist>thresh_val]\n",
    "        \n",
    "        if len(dendrite_xyz)==0:\n",
    "            dendrite_xyz=np.zeros((1,3))#float(\"nan\")\n",
    "        if len(axon_xyz)==0:\n",
    "            axon_xyz=np.zeros((1,3))#float(\"nan\")\n",
    "        return (dendrite_xyz, axon_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider there exist connection between axon terminal of neuron 1 and dendritic endpoint of neuron 2 \n",
    "# only if the euclidean distance between these two is less than or equal to proximity range( say 5 um ).\n",
    "def count_connections(axon_xyz1, dendrite_xyz2, synaptic_cleft):\n",
    "    if np.all(axon_xyz1==0) or np.all(dendrite_xyz2==0):\n",
    "        \n",
    "        return \"NA\"\n",
    "    else:\n",
    "        dist = distance.cdist(axon_xyz1, dendrite_xyz2)\n",
    "        #print(dist)\n",
    "        #print()\n",
    "        connections = np.count_nonzero(dist <= synaptic_cleft)\n",
    "        if connections==0:\n",
    "            return \"NA\"\n",
    "        else:\n",
    "            return connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71733a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01efe01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelise using multiprocess to store edges in a text file.\n",
    "\n",
    "import multiprocess\n",
    "from functools import partial\n",
    "\n",
    "def write_line_temp(i, pr, nodes, df_neuron_attr, path, new_swc_dir):\n",
    "    node1 = nodes[i]\n",
    "    synaptic_cleft = pr*1000/8 # 1000/8 is a factor used to keep in um(micrometer) dimension. \n",
    "    j_list = list(range(i+1, len(nodes)))\n",
    "\n",
    "    for j in j_list:\n",
    "        #print(node1, node2)\n",
    "        node2 = nodes[j]\n",
    "\n",
    "        dendrite_xyz1, axon_xyz1 = get_endpoints(node1, df_neuron_attr, path+new_swc_dir)\n",
    "        dendrite_xyz2, axon_xyz2 = get_endpoints(node2, df_neuron_attr, path+new_swc_dir)\n",
    "\n",
    "        weight_12 = count_connections(axon_xyz1, dendrite_xyz2, synaptic_cleft)\n",
    "        if weight_12!=\"NA\":\n",
    "            line = str(node1) + \"\\t\" + str(node2) + \"\\t\" + str(weight_12) + \"\\n\"\n",
    "            with open(path+\"network_%.1fum.txt\"%pr, \"a\") as file:\n",
    "                file.write(line)\n",
    "\n",
    "        weight_21 = count_connections(axon_xyz2, dendrite_xyz1, synaptic_cleft)\n",
    "        if weight_21!=\"NA\":\n",
    "            line = str(node2) + \"\\t\" + str(node1) + \"\\t\" + str(weight_21) + \"\\n\"\n",
    "            with open(path+\"network_%.1fum.txt\"%pr, \"a\") as file:\n",
    "                file.write(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219166aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the reconstructed subnetworks corresponding to communities from EM drosophilia\n",
    "# which uses strahler numbering, threshold distance and proximity range.\n",
    "# networks with 2 proximity sizes are stored i.e. 1um and 5um.\n",
    "\n",
    "for j in range(8):\n",
    "    print(\"Comm %d\"%j)\n",
    "    for pr in [1.0, 5.0]:\n",
    "        print(\"prox range: %.1f um\"%pr)\n",
    "        start = time.time()\n",
    "        \n",
    "        nodes = pd.read_csv(\"./EM_communities/averageSubComm%d_drosophilia.csv\"%j).Node.to_list()\n",
    "        path = \"./janelia_Comm%d/\"%j\n",
    "        new_swc_dir = \"janelia dataset with strahler lastcolumn/\"\n",
    "\n",
    "        df_neuron_attr=pd.read_csv(path+\"neu_attrJanelia_updated.csv\")\n",
    "\n",
    "        #node_wSoma = df_neuron_attr[~np.isnan( df_neuron_attr.soma )].id.tolist()\n",
    "\n",
    "        i_list = list(range(len(nodes)))\n",
    "\n",
    "       \n",
    "        pool = multiprocess.Pool()\n",
    "        write_line=partial(write_line_temp, pr = pr, nodes = nodes, \\\n",
    "                       df_neuron_attr=df_neuron_attr, path=path, new_swc_dir=new_swc_dir)\n",
    "\n",
    "        pool.map(write_line, i_list)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "       \n",
    "        end = time.time()\n",
    "        print(\"Total time: %.2f hrs\\n\"%((end-start)/3600) )\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e799cef-4a1b-4f95-8325-6c0d84bc7640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c57d6897-8ec2-4630-9b48-ebfcc4ce1f87",
   "metadata": {},
   "source": [
    "# Community detection applied to reconstructed subnetworks 0 to 7.\n",
    "\n",
    "### Leiden algorithm is applied to all the reconstructed subnetworks to determine the modules within each of these subnetworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f47369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import leidenalg\n",
    "import igraph as ig\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bab037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################   All proximity ranges    ###################################\n",
    "for j in range(8):\n",
    "    print(\"Comm %d\"%j)\n",
    "    for pr in [1.0, 5.0]:\n",
    "        print(\"prox range: %.1f um\"%pr)\n",
    "        \n",
    "        nodes = pd.read_csv(\"./EM_communities/averageSubComm%d_drosophilia.csv\"%j).Node.to_list()\n",
    "        path = \"./janelia_Comm%d/\"%j\n",
    "        \n",
    "    \n",
    "        df_network = pd.read_csv(path+\"network_%.1fum.txt\"%pr, sep=\"\\t\", header=None)\n",
    "        df_network = df_network.rename(columns={0: \"bodyId_pre\", 1: \"bodyId_post\", 2:\"weight\"})\n",
    "\n",
    "        nodeA_list = df_network.bodyId_pre.tolist()\n",
    "        nodeB_list = df_network.bodyId_post.tolist()\n",
    "        weight_list = df_network.weight.tolist()\n",
    "\n",
    "\n",
    "        # creating an edge list from adjacency matrix\n",
    "        edge_list=[]\n",
    "        for i,j,w in zip(nodeA_list, nodeB_list, weight_list):\n",
    "            edge_list.append( (i,j,{\"weight\":w}))#, \"Label\":\"%s - %s\"%(nodes[i],nodes[j])}  ))\n",
    "\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        # Add all the nodes to the graph\n",
    "        G.add_nodes_from(nodes)\n",
    "        # Add all the edges to the graph\n",
    "        G.add_edges_from(edge_list)\n",
    "\n",
    "        \n",
    "        def find_number_communities(graph):\n",
    "            nComm = []\n",
    "            for i in range(30):\n",
    "                comm_membership = leidenalg.find_partition(graph, leidenalg.ModularityVertexPartition, weights=\"weight\")\n",
    "    \n",
    "                nComm.append(len(np.unique(comm_membership.membership)))\n",
    "    \n",
    "            return Counter(nComm).most_common(1)[0][0]\n",
    "    \n",
    "        import concurrent.futures.ProcessPoolExecutor\n",
    "    \n",
    "        def find_communities(list_communities, idx_communities):\n",
    "            comm = []\n",
    "            for idx in idx_communities:\n",
    "                comm.append(np.where(np.array(list_communities) == idx)[0])\n",
    "            return comm\n",
    "    \n",
    "        def find_overlap(comm_check, comm_truth):\n",
    "            overlap = np.zeros((len(comm_truth), len(comm_check)), dtype = np.float64)\n",
    "            for i, comm_i in enumerate(comm_truth):\n",
    "                for j, comm_j in enumerate(comm_check):\n",
    "                    overlap[i][j] = np.intersect1d(comm_i, comm_j).size/(comm_i.size + comm_j.size)\n",
    "            return overlap\n",
    "    \n",
    "        def relabel_communities(idx_comm, comm_truth, list_communities, idx_communities):\n",
    "            old_comm_list = list_communities[idx_comm]\n",
    "            overlap = find_overlap(find_communities(old_comm_list, idx_communities),\n",
    "                                   comm_truth)\n",
    "    \n",
    "            new_idxs = np.argmax(overlap, axis = 0)\n",
    "            new_comm_list = np.empty(len(old_comm_list))\n",
    "    \n",
    "            for idx, old_val in enumerate(old_comm_list):\n",
    "                new_comm_list[idx] = new_idxs[old_val]\n",
    "    \n",
    "            return new_comm_list\n",
    "    \n",
    "        def Community_save(Graph, membership, filename):    \n",
    "            df = pd.DataFrame()\n",
    "            df['Node'] = Graph.vs['_nx_name']\n",
    "            df['Community'] = membership\n",
    "            #df['Name'] = Graph.vs['name']\n",
    "    \n",
    "            df.to_csv(filename+\".csv\", index=False)\n",
    "    \n",
    "        G_ig = ig.Graph.from_networkx(G)\n",
    "    \n",
    "        num_comm_tofind = find_number_communities(G_ig)\n",
    "    \n",
    "        def communities_loop(args):\n",
    "            nreps, seed = args\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "            list_communities = []\n",
    "            while len(list_communities) < nreps:\n",
    "                dir_leiden = leidenalg.find_partition(G_ig, leidenalg.ModularityVertexPartition, weights=\"weight\")\n",
    "                if np.unique(dir_leiden.membership).size == num_comm_tofind:\n",
    "                    list_communities.append(dir_leiden.membership)\n",
    "    \n",
    "            return list_communities\n",
    "    \n",
    "        nloops = 32\n",
    "        seeds = np.random.randint(0, int(1e6), size = nloops)\n",
    "    \n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        #pool = ProcessPoolExecutor(max_workers=3)\n",
    "            parallel_res = executor.map(communities_loop,\n",
    "                                        zip([32]*nloops,\n",
    "                                            seeds))\n",
    "                #parallel_res = parallel_res.result()\n",
    "            #executor.shutdown(wait=False, cancel_futures=True)    \n",
    "    \n",
    "        list_communities = []\n",
    "        for res in parallel_res:\n",
    "            [list_communities.append(i) for i in res]\n",
    "    \n",
    "        #return list_communities\n",
    "        idx_communities = np.array(list(range(num_comm_tofind)))\n",
    "        comm_truth = find_communities(list_communities[0], idx_communities)\n",
    "    \n",
    "        relabeled_comm_list = []\n",
    "        for idx in range(len(list_communities)):\n",
    "            relabeled_comm_list.append(relabel_communities(idx, comm_truth,\n",
    "                                                           list_communities, idx_communities))\n",
    "    \n",
    "        relabeled_comm_list = np.array(relabeled_comm_list).astype(np.int8)\n",
    "    \n",
    "        final_communities = np.empty(relabeled_comm_list.shape[1])\n",
    "        for idx, rep in enumerate(relabeled_comm_list.T):\n",
    "            final_communities[idx] = np.bincount(rep).argmax()\n",
    "    \n",
    "\n",
    "        Community_save(G_ig, final_communities,\\\n",
    "                       path+\"SubComm%d_reconstructed_%.1fum\"%(d,pr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5f0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
